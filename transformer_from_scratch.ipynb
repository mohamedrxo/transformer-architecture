{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I8oY_wMtRhNz"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "THIS FILE IS  IMPLEMENTATION OF THE TRANSFORMER MODEL FROM SCRATCH WHITOUT DEPENDING ON PYTORCH PRE-BUILD CLASSES   AND THE TRAINING PROCCESS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK7iQcCBS5LO",
        "outputId": "a14335b9-85f7-491c-9a8a-0aaca20d0d7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhzXu-v2Rial",
        "outputId": "aa076681-8812-43d8-fdab-de0fabc75d43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import tiktoken\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IwxDZ0yHYCaQ"
      },
      "outputs": [],
      "source": [
        "# multihead attention from scratch\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, device=None):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dk = d_model // num_heads\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False).to(self.device)\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False).to(self.device)\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False).to(self.device)\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False).to(self.device)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask=None, padding_mask=None):\n",
        "        dk = query.size(-1)\n",
        "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(dk, dtype=torch.float32, device=query.device))\n",
        "\n",
        "        if padding_mask is not None:\n",
        "            padding_mask = padding_mask.unsqueeze(1).unsqueeze(1)\n",
        "            attention_scores = attention_scores.masked_fill(padding_mask == 0, float('-inf'))\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).unsqueeze(1)\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "        attention_output = torch.matmul(attention_probs, value)\n",
        "\n",
        "        return attention_output\n",
        "\n",
        "    def forward(self, query, key, value, mask=None, padding_mask=None):\n",
        "        batch_size = key.size(0)\n",
        "        key_len, query_len = key.size(1), query.size(1)\n",
        "\n",
        "        k = self.w_k(key)\n",
        "        q = self.w_q(query)\n",
        "        v = self.w_v(value)\n",
        "\n",
        "        k = k.view(batch_size, key_len, self.num_heads, self.dk).transpose(1, 2)\n",
        "        q = q.view(batch_size, query_len, self.num_heads, self.dk).transpose(1, 2)\n",
        "        v = v.view(batch_size, key_len, self.num_heads, self.dk).transpose(1, 2)\n",
        "\n",
        "        attention_output = self.attention(q, k, v, mask, padding_mask)\n",
        "\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, query_len, self.d_model)\n",
        "        output = self.w_o(attention_output)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2PQXVPCxRkXh"
      },
      "outputs": [],
      "source": [
        "# the positional encoding add the positining of word in sentence\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout, max_len=5000, device=None):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        pe = torch.zeros(max_len, d_model)  # No device specified here\n",
        "\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe.to(self.device))  # Move pe to device\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BUQzK8ZCRpc7"
      },
      "outputs": [],
      "source": [
        "# the encoder block that use the multihead attention plus resutial connnection\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, device=None):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Initialize components\n",
        "        self.sa = MultiHeadAttention(n_embd, n_head, device=self.device)\n",
        "        self.ffwd = FeedForward(n_embd, device=self.device)  # Ensure FeedForward is CUDA-compatible\n",
        "        self.ln1 = nn.LayerNorm(n_embd).to(self.device)\n",
        "        self.ln2 = nn.LayerNorm(n_embd).to(self.device)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Ensure input tensor x is on the correct device\n",
        "        x = x.to(self.device)\n",
        "\n",
        "\n",
        "        # Apply self-attention, add residual, and apply layer normalization\n",
        "        x = x + self.ln1(self.sa(x, x, x, mask))\n",
        "\n",
        "        # Apply feed-forward network, add residual, and apply layer normalization\n",
        "        x = x + self.ln2(self.ffwd(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hg1LnMhdRslo"
      },
      "outputs": [],
      "source": [
        "# the encoder repeat the proccess of EncoderBlock multiple times\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, n_layer, device=None):\n",
        "        super().__init__()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Create a list of EncoderBlock layers\n",
        "        self.layers = nn.ModuleList([EncoderBlock(n_embd, n_head, device=self.device) for _ in range(n_layer)])\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Ensure input tensor x is on the correct device\n",
        "        x = x.to(self.device)\n",
        "\n",
        "        # Pass through each EncoderBlock\n",
        "        for layer in self.layers:\n",
        "\n",
        "            x = layer(x, mask)\n",
        "\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1wTDChxsRuho"
      },
      "outputs": [],
      "source": [
        "# this is the decoder part of the transformer\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, device=None):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Initialize components\n",
        "        self.mutli_head_attention = MultiHeadAttention(n_embd, 4, device=self.device)\n",
        "        self.cross_attention = MultiHeadAttention(n_embd, 4, device=self.device)\n",
        "        self.ffwd = FeedForward(n_embd, device=self.device)  # Ensure FeedForward is CUDA-compatible\n",
        "    def forward(self, x, encoder_output,padding_mask):\n",
        "      mask = torch.tril(torch.ones(1, x.shape[1])).to(self.device)\n",
        "\n",
        "      x = x+self.mutli_head_attention(x,x,x,mask,padding_mask)\n",
        "\n",
        "      x = x+self.cross_attention(x,encoder_output,encoder_output)\n",
        "      x = x + self.ffwd(x)\n",
        "      return x\n",
        "        # Ensure input tensor x is on the correct device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AkjBRlRiRvgQ"
      },
      "outputs": [],
      "source": [
        "# the decoder part of the transformer\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, n_layer, device=None):\n",
        "        super().__init__()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Create a list of EncoderBlock layers\n",
        "        self.layers = nn.ModuleList([DecoderBlock(n_embd, n_head, device=self.device) for _ in range(n_layer)])\n",
        "        self.n1  = nn.LayerNorm(n_embd).to(self.device)\n",
        "    def forward(self, x, encoder_output,mask=None):\n",
        "        # Ensure input tensor x is on the correct device\n",
        "        x = x.to(self.device)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x,encoder_output,mask)\n",
        "        x = self.n1(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PJTsVopOR0fi"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd, device=None, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Define the feed-forward network with ReLU activation and dropout\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_prob),  # Apply dropout after ReLU\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "        ).to(self.device)  # Move the network to the specified device\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EXJqlIfFR0xr"
      },
      "outputs": [],
      "source": [
        "# feed forward to map emdadding to vocab size\n",
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x) -> None:\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "        return self.proj(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oBbVFCG_YH6X"
      },
      "outputs": [],
      "source": [
        "# the full implementation fo the transformer model using only the elemnt we have build before\n",
        "import torch.nn.functional as f\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, n_head, n_layer, device=None):\n",
        "        super().__init__()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.sqrt = n_embd**0.5\n",
        "        # Initialize components\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd).to(self.device)\n",
        "        self.positional_encoding = PositionalEncoding(n_embd,0.2)\n",
        "        self.encoder = Encoder(n_embd, n_head, n_layer, device=self.device)\n",
        "        self.decoder = Decoder(n_embd, n_head, n_layer, device=self.device)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size).to(self.device)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.project  = ProjectionLayer(n_embd,vocab_size)\n",
        "\n",
        "    def encoder_layer(self,x,mask):\n",
        "        out =  self.encoder(x,mask)\n",
        "        return out\n",
        "    def decoder_layer(self,x,encoder_output,mask):\n",
        "        out =  self.decoder(x,encoder_output,mask)\n",
        "        return out\n",
        "    def embadding(self,src):\n",
        "         return  self.token_embedding_table(src)*self.sqrt\n",
        "\n",
        "    def forward(self, src,target, mask=None):\n",
        "      encoder_padding=(src != 50258).float()\n",
        "\n",
        "      src = self.embadding(src)\n",
        "\n",
        "      src  = self.positional_encoding(src)\n",
        "      decoder_padding  = (target != 50258).float()\n",
        "      target = self.embadding(target)\n",
        "\n",
        "      target = self.positional_encoding(target)\n",
        "\n",
        "\n",
        "      encoder_output = self.encoder_layer(src,encoder_padding)\n",
        "      decoder_output = self.decoder_layer(target,encoder_output,decoder_padding)\n",
        "      logits = self.project(decoder_output)\n",
        "\n",
        "      return logits\n",
        "    def inference(self, src, mask=None):\n",
        "\n",
        "        tgt=torch.tensor([[50256]],device=self.device)\n",
        "        src=torch.cat((torch.tensor([[50256]]), torch.tensor( [enc.encode(src)]),torch.tensor([[50257]])),dim=1).to(self.device)\n",
        "\n",
        "        src= self.embadding(src)\n",
        "        src=self.positional_encoding(src)\n",
        "        out_encoder=self.encoder(src)\n",
        "\n",
        "\n",
        "        while True:\n",
        "\n",
        "            tgts= self.embadding(tgt)\n",
        "            tgts=self.positional_encoding(tgts)\n",
        "            logits=self.decoder(tgts,out_encoder)\n",
        "            logits = self.project(logits)\n",
        "            logits=logits[:,-1,:]\n",
        "\n",
        "\n",
        "            logits = logits.view(-1)\n",
        "            probs= f.softmax(logits,dim=0)\n",
        "\n",
        "            prediction  = torch.argmax(probs)\n",
        "            next_tgts=prediction.reshape(1,1)\n",
        "            tgt=torch.cat((tgt,next_tgts),dim=1)\n",
        "\n",
        "\n",
        "            print(enc.decode(tgt.squeeze(0).tolist()))\n",
        "            if next_tgts.item()==50257:\n",
        "                print(enc.decode(tgt.squeeze(0).tolist()))\n",
        "                break\n",
        "        return tgt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0roegVWR3Sj",
        "outputId": "3ba9ec63-80dc-4c07-c4d0-dfe1b3fe0954"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# this is huggingface librarie for datasets \n",
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM5ZYb6uSCak",
        "outputId": "32e06f74-a474-4ebf-d884-46169c175724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3SQwPUGBR8sF"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "F1tEIw8QR_UC"
      },
      "outputs": [],
      "source": [
        "# we use the same encoding as gpt 2 \n",
        "enc = tiktoken.encoding_for_model(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tFh3_rG-SEC-"
      },
      "outputs": [],
      "source": [
        "enc = tiktoken.Encoding(\n",
        "\n",
        "    name=\"gpt2\",\n",
        "    pat_str=enc._pat_str,\n",
        "    mergeable_ranks=enc._mergeable_ranks,\n",
        "    special_tokens={\n",
        "        \"<sos>\": 50256,\n",
        "        \"<eos>\": 50257,\n",
        "        \"<pad>\":50258\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fhEctjMSHpV",
        "outputId": "962f7480-f149-4cdf-b0d6-6ed0db75e81a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[50256,    51,  8056,   811,     0, 50257],\n",
              "         [50256,  5189,  1781,    13, 50257, 50258]], device='cuda:0'),\n",
              " tensor([[50256,    38, 35942,   498,   447,   107,     0],\n",
              "         [50256,    33,  2013,   264, 42324,    81,    13]], device='cuda:0'),\n",
              " tensor([[   38, 35942,   498,   447,   107,     0, 50257],\n",
              "         [   33,  2013,   264, 42324,    81,    13, 50257]], device='cuda:0'))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "raw_dataset = load_dataset(\"PaulineSanchez/Translation_words_and_sentences_english_french\")['train'][:1000]\n",
        "\n",
        "# Custom PyTorch Dataset to interface with the Hugging Face dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "\n",
        "\n",
        "        return {\"english\": enc.encode(self.dataset[\"English words/sentences\"][idx],allowed_special=\"all\"), \"french\":enc.encode(self.dataset[\"French words/sentences\"][idx],allowed_special=\"all\")}\n",
        "\n",
        "# # Create the custom dataset\n",
        "custom_dataset = CustomDataset(raw_dataset)\n",
        "custom_dataset[0]\n",
        "\n",
        "# # Define the DataLoader\n",
        "batch_size = 32\n",
        "data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "train_data = CustomDataset(raw_dataset)\n",
        "\n",
        "# # Function to generate random batches\n",
        "def get_batch(batch_size=2):\n",
        "    \"\"\"\n",
        "    Generate random batches with sequences dynamically padded to the same length.\n",
        "    \"\"\"\n",
        "    # Randomly sample indices\n",
        "    idx = torch.randint(1000, (batch_size,))\n",
        "\n",
        "    # Encode the batches\n",
        "    english_batch = [\n",
        "        torch.tensor(train_data[i.item()]['english'])\n",
        "        for i in idx\n",
        "    ]\n",
        "    french_batch = [\n",
        "        torch.tensor(train_data[i.item()]['french'])\n",
        "        for i in idx\n",
        "    ]\n",
        "\n",
        "    # Add special tokens (50258 for start, 50259 for end)\n",
        "    trainer = [torch.cat((torch.tensor([50256]), tensor, torch.tensor([50257]))) for tensor in english_batch]\n",
        "    decoder = [torch.cat((torch.tensor([50256]), tensor)) for tensor in french_batch]\n",
        "    out = [torch.cat((tensor, torch.tensor([50257]))) for tensor in french_batch]\n",
        "\n",
        "    # Determine the maximum sequence length for padding\n",
        "    max_len = max(\n",
        "        max(len(tensor) for tensor in trainer),\n",
        "        max(len(tensor) for tensor in decoder),\n",
        "        max(len(tensor) for tensor in out)\n",
        "    )\n",
        "\n",
        "    # Pad sequences to the maximum length\n",
        "    input = pad_sequence(trainer, batch_first=True, padding_value=50258)\n",
        "    output = pad_sequence(decoder, batch_first=True, padding_value=50258)\n",
        "    out = pad_sequence(out, batch_first=True, padding_value=50258)\n",
        "\n",
        "    return input.to(device), output.to(device), out.to(device)\n",
        "get_batch()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dIQS2JbeSO15"
      },
      "outputs": [],
      "source": [
        "# intialise the transformer model\n",
        "english_french_transformer =  Transformer(enc.n_vocab,512,8,8)\n",
        "encoder,decoder,target  =  get_batch()\n",
        "mask = torch.tril(torch.ones((encoder.shape[1], encoder.shape[1])))\n",
        "english_french_transformer = english_french_transformer.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otY3U_fmb7Ks",
        "outputId": "10659290-a0d1-49c2-a578-b7e693cdf396"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "136076966\n"
          ]
        }
      ],
      "source": [
        "# calculate the number of parameters in the model\n",
        "pytorch_total_params = sum(p.numel() for p in english_french_transformer.parameters())\n",
        "print(pytorch_total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "HWtVB7RrbX9m"
      },
      "outputs": [],
      "source": [
        "# enpty the gpu\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# i would recommand that you use weight and baies to visualise you data progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "Pi0-iDlgSW5K",
        "outputId": "c6b599a4-c83f-47f6-a04a-48b91abeb6cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  16%|█▋        | 165/1000 [00:19<01:38,  8.44batch/s, Loss=1.93]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-93e802a50bb2>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Dynamically display the loss at each iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.AdamW(english_french_transformer.parameters(), lr=1e-4)\n",
        "# vocab_size = 50\n",
        "# Wrap the loop with tqdm for progress bar\n",
        "progress_bar = tqdm(range(1000), desc=\"Training Progress\", unit=\"batch\")\n",
        "english_french_transformer.train()\n",
        "for i in progress_bar:\n",
        "    x , y , z = get_batch(30)  # Get a batch of training data\n",
        "    logits= english_french_transformer(x, y)  # Forward pass\n",
        "\n",
        "    B , T,C = logits.shape\n",
        "    logits = logits.view(B*T, C)\n",
        "    targets = z.view(B*T)\n",
        "    loss = F.cross_entropy(logits, targets)\n",
        "    optimizer.zero_grad(set_to_none=True)  # Reset gradients\n",
        "    loss.backward()  # Backward pass\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    # Dynamically display the loss at each iteration\n",
        "    progress_bar.set_postfix({\"Loss\": loss.item()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5UJpi7fXCKL",
        "outputId": "92787458-3494-4150-96d2-917a580bf70c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([40932,   279,  1072,   627,     6,   346,  1556,  2169,   862, 12797,\n",
            "         6941,    72,   288,     6,  3258, 25792,   353, 12797,   257, 23577,\n",
            "          454,    67,     6,    71,  9019,    13, 50257], device='cuda:0') tensor([[-5.5289, -4.9186, -4.6765,  ..., -5.9165,  8.0270, -4.5526],\n",
            "        [-5.5071, -4.9121, -4.8158,  ..., -5.8610,  7.9478, -4.5072],\n",
            "        [-5.5314, -4.9203, -4.7465,  ..., -5.8397,  7.9506, -4.5072],\n",
            "        ...,\n",
            "        [-5.5570, -4.9310, -4.7348,  ..., -5.8437,  7.9310, -4.5126],\n",
            "        [-5.6017, -4.9109, -4.7667,  ..., -5.8244,  7.9770, -4.5137],\n",
            "        [-5.6004, -4.8817, -4.7502,  ..., -5.8522,  7.9142, -4.5155]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(targets,logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccN7f3ePSjpA",
        "outputId": "13a259d9-62b6-4a13-963f-8cb1cb23c41e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<sos>Stop that!<eos><pad>\n",
            "<sos>Arrêtez !<pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "Arrêtez !<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "<sos>C\n",
            "<sos>C'\n",
            "<sos>C'est\n",
            "<sos>C'est-\n",
            "<sos>C'est-mo\n",
            "<sos>C'est-moi\n",
            "<sos>C'est-moi !\n",
            "<sos>C'est-moi !<eos>\n",
            "<sos>C'est-moi !<eos>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[50256,    34,     6,   395,    12,  5908,    72,  5145, 50257]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# use this  for inference after training\n",
        "english_french_transformer.eval()\n",
        "print(enc.decode(x[0].tolist()))\n",
        "print(enc.decode(y[0].tolist()))\n",
        "print(enc.decode(z[0].tolist()))\n",
        "sentence =enc.decode(x[0].tolist())\n",
        "english_french_transformer.inference(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCAFsoBpmBR5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "ff408857cf941f447a225a9fe246d3ffac9d263a43d426dbe48fb0523ecbaef2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
